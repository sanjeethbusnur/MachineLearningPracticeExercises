{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDL_Assignment_4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GyI7T-t5gMUo",
        "9to98r7NKbO1",
        "5tBkcP6sUmGz",
        "nqxDawCrquJb"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw6c1bWxWgjG",
        "colab_type": "text"
      },
      "source": [
        "**Deep Learning programming task**\n",
        "\n",
        "Assignment 4 : Dense Net and tf.function\n",
        "\n",
        "Team members:\n",
        "\n",
        "Sanjeeth Busnur Indushekar: 224133 : sanjeeth.busnur@st.ovgu.de\n",
        "\n",
        "\n",
        "Aditya Dey : 230580 : aditya.dey@st.ovgu.de\n",
        "\n",
        "\n",
        "Suraj Shashidhar: 230052 : suraj.shashidhar@st.ovgu.de"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjzMvmC6NnSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models,optimizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KryrKrC8diG3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "3b119669-f2e6-418b-970f-23df3a651a02"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1m1Ff1OdqFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQO0M8Zfdt-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/Colab Notebooks')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKE97d8sQNPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization\n",
        "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import Input, Flatten, Dropout\n",
        "from tensorflow.keras.layers import concatenate, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyI7T-t5gMUo",
        "colab_type": "text"
      },
      "source": [
        "### Cifar10 data using tf.function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtQ6fgx-OOdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgA9j7NeOd3e",
        "colab_type": "code",
        "outputId": "e232c56c-a025-417e-f0c4-923fc6f72cb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "print(train_labels[10])\n",
        "plt.imshow(train_images[10], cmap=plt.cm.binary)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe504c54898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "319HvHfkO82r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices(( train_images.astype(np.float32)/255.0, train_labels.astype(np.int32) ))\n",
        "\n",
        "train_data = train_data.shuffle(buffer_size = train_images.shape[0]).batch(256)\n",
        "\n",
        "test_data = tf.data.Dataset.from_tensor_slices(( test_images.astype(np.float32)/255.0, test_labels.astype(np.int32) ))\n",
        "\n",
        "test_data = test_data.batch(256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiVwYVjsPm6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.Sequential([layers.Conv2D(64,(3,3),activation='relu',padding='same',kernel_initializer= \"he_uniform\",input_shape=(32,32,3)),\n",
        "                          layers.BatchNormalization(),\n",
        "                          layers.Conv2D(64,(3,3),activation='relu',padding='same',kernel_initializer= \"he_uniform\"),\n",
        "                          layers.BatchNormalization(), \n",
        "                          layers.MaxPooling2D(),\n",
        "                          layers.Dropout(0.3),\n",
        "                           \n",
        "                          layers.Conv2D(128,(3,3),activation='relu',padding='same',kernel_initializer= \"he_uniform\"),\n",
        "                          layers.BatchNormalization(),\n",
        "                          layers.Conv2D(128,(3,3),activation='relu',padding='same',kernel_initializer= \"he_uniform\"),\n",
        "                          layers.BatchNormalization(), \n",
        "                          layers.MaxPooling2D(),\n",
        "                          layers.Dropout(0.4),\n",
        "\n",
        "                          layers.Conv2D(256,(3,3),activation='relu',padding='same',kernel_initializer= \"he_uniform\"),\n",
        "                          layers.BatchNormalization(),\n",
        "                          layers.Conv2D(256,(3,3),activation='relu',padding='same',kernel_initializer= \"he_uniform\"),\n",
        "                          layers.BatchNormalization(), \n",
        "                          layers.MaxPooling2D(),\n",
        "                          layers.Dropout(0.5), \n",
        "                           \n",
        "                          layers.Flatten(), \n",
        "                          layers.Dense(64, activation='relu'), \n",
        "                          layers.Dense(10) ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrwM2Ae5RsIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=  5 * 1e-3)\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oc4G6IdR1xE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @tf.function\n",
        "def train_step(imgs, lbls):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(imgs)\n",
        "        xent = loss_fn(lbls, logits)\n",
        "\n",
        "    varis = model.trainable_variables\n",
        "    grads = tape.gradient(xent, varis)\n",
        "    optimizer.apply_gradients(zip(grads, varis))\n",
        "\n",
        "    return xent, logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhryqteYR_ZO",
        "colab_type": "code",
        "outputId": "59f7840c-211a-405a-8010-2246751c24c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "start = time.time()\n",
        "epochs=21\n",
        "for epoch in range(epochs):\n",
        "  tf.print(\"===== epoch number: {}\".format(epoch))\n",
        "\n",
        "  start_epoch = time.time()\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_data):\n",
        "    xent, logits = train_step(x_batch_train, y_batch_train)\n",
        "    train_acc_metric(y_batch_train, logits)\n",
        "\n",
        "  train_acc = train_acc_metric.result()\n",
        "  tf.print('Training acc over epoch: %s' % (float(train_acc),))\n",
        "  train_acc_metric.reset_states()\n",
        "\n",
        "  for x_batch_val, y_batch_val in test_data:\n",
        "    val_logits = model(x_batch_val)\n",
        "    val_acc_metric(y_batch_val, val_logits)\n",
        "  val_acc = val_acc_metric.result()\n",
        "  val_acc_metric.reset_states()\n",
        "  tf.print('Validation acc: %s' % (float(val_acc),))\n",
        "  end_epoch = time.time()\n",
        "  tf.print(\"time taken for this epoch(in seconds): \",(end_epoch - start_epoch))\n",
        "stop = time.time()\n",
        "tf.print(\"total time(in second): \", (stop - start))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== epoch number: 0\n",
            "Training acc over epoch: 0.23216000199317932\n",
            "Validation acc: 0.3165999948978424\n",
            "time taken for this epoch(in seconds):  9.4675772190094\n",
            "===== epoch number: 1\n",
            "Training acc over epoch: 0.40525999665260315\n",
            "Validation acc: 0.4715999960899353\n",
            "time taken for this epoch(in seconds):  8.317382335662842\n",
            "===== epoch number: 2\n",
            "Training acc over epoch: 0.51774001121521\n",
            "Validation acc: 0.5710999965667725\n",
            "time taken for this epoch(in seconds):  8.367700815200806\n",
            "===== epoch number: 3\n",
            "Training acc over epoch: 0.5910400152206421\n",
            "Validation acc: 0.6248999834060669\n",
            "time taken for this epoch(in seconds):  8.313086748123169\n",
            "===== epoch number: 4\n",
            "Training acc over epoch: 0.651960015296936\n",
            "Validation acc: 0.6299999952316284\n",
            "time taken for this epoch(in seconds):  8.295811414718628\n",
            "===== epoch number: 5\n",
            "Training acc over epoch: 0.6941400170326233\n",
            "Validation acc: 0.652999997138977\n",
            "time taken for this epoch(in seconds):  8.30174732208252\n",
            "===== epoch number: 6\n",
            "Training acc over epoch: 0.7298799753189087\n",
            "Validation acc: 0.6783000230789185\n",
            "time taken for this epoch(in seconds):  8.36907172203064\n",
            "===== epoch number: 7\n",
            "Training acc over epoch: 0.7577999830245972\n",
            "Validation acc: 0.6916999816894531\n",
            "time taken for this epoch(in seconds):  8.31182861328125\n",
            "===== epoch number: 8\n",
            "Training acc over epoch: 0.7886599898338318\n",
            "Validation acc: 0.6970000267028809\n",
            "time taken for this epoch(in seconds):  8.298018217086792\n",
            "===== epoch number: 9\n",
            "Training acc over epoch: 0.8158000111579895\n",
            "Validation acc: 0.6944000124931335\n",
            "time taken for this epoch(in seconds):  8.33131742477417\n",
            "===== epoch number: 10\n",
            "Training acc over epoch: 0.8367199897766113\n",
            "Validation acc: 0.6984999775886536\n",
            "time taken for this epoch(in seconds):  8.306721687316895\n",
            "===== epoch number: 11\n",
            "Training acc over epoch: 0.8584200143814087\n",
            "Validation acc: 0.6915000081062317\n",
            "time taken for this epoch(in seconds):  8.326209306716919\n",
            "===== epoch number: 12\n",
            "Training acc over epoch: 0.8804600238800049\n",
            "Validation acc: 0.6870999932289124\n",
            "time taken for this epoch(in seconds):  8.308214902877808\n",
            "===== epoch number: 13\n",
            "Training acc over epoch: 0.8900399804115295\n",
            "Validation acc: 0.6980999708175659\n",
            "time taken for this epoch(in seconds):  8.364709377288818\n",
            "===== epoch number: 14\n",
            "Training acc over epoch: 0.9073200225830078\n",
            "Validation acc: 0.6998000144958496\n",
            "time taken for this epoch(in seconds):  8.333881855010986\n",
            "===== epoch number: 15\n",
            "Training acc over epoch: 0.9231399893760681\n",
            "Validation acc: 0.6866000294685364\n",
            "time taken for this epoch(in seconds):  8.315820693969727\n",
            "===== epoch number: 16\n",
            "Training acc over epoch: 0.9269999861717224\n",
            "Validation acc: 0.6949999928474426\n",
            "time taken for this epoch(in seconds):  8.332215070724487\n",
            "===== epoch number: 17\n",
            "Training acc over epoch: 0.9408599734306335\n",
            "Validation acc: 0.6967999935150146\n",
            "time taken for this epoch(in seconds):  8.401806354522705\n",
            "===== epoch number: 18\n",
            "Training acc over epoch: 0.9440600275993347\n",
            "Validation acc: 0.6988000273704529\n",
            "time taken for this epoch(in seconds):  8.316762208938599\n",
            "===== epoch number: 19\n",
            "Training acc over epoch: 0.9466400146484375\n",
            "Validation acc: 0.6992999911308289\n",
            "time taken for this epoch(in seconds):  8.36726999282837\n",
            "===== epoch number: 20\n",
            "Training acc over epoch: 0.9526399970054626\n",
            "Validation acc: 0.6945000290870667\n",
            "time taken for this epoch(in seconds):  8.344850540161133\n",
            "total time(in second):  176.19322299957275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8zyeLh2VuFw",
        "colab_type": "code",
        "outputId": "c66bd114-71c0-4189-d32f-f754a52f5520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for img_batch, lbl_batch in test_data:\n",
        "    val_acc_metric(lbl_batch, model(img_batch))\n",
        "\n",
        "val_acc_metric.result()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.6945>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4MAm5xmgCY4",
        "colab_type": "text"
      },
      "source": [
        "**With out tf.function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTic-XnTaEyM",
        "colab_type": "code",
        "outputId": "60b63f22-bbd5-4589-ed0e-e8c1c81c85e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "start = time.time()\n",
        "epochs=21\n",
        "for epoch in range(epochs):\n",
        "  print(\"===== epoch number: {}\".format(epoch))\n",
        "\n",
        "  start_epoch = time.time()\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_data):\n",
        "    xent, logits = train_step(x_batch_train, y_batch_train)\n",
        "    train_acc_metric(y_batch_train, logits)\n",
        "\n",
        "  train_acc = train_acc_metric.result()\n",
        "  print('Training acc over epoch: %s' % (float(train_acc),))\n",
        "  train_acc_metric.reset_states()\n",
        "\n",
        "  for x_batch_val, y_batch_val in test_data:\n",
        "    val_logits = model(x_batch_val)\n",
        "    val_acc_metric(y_batch_val, val_logits)\n",
        "  val_acc = val_acc_metric.result()\n",
        "  val_acc_metric.reset_states()\n",
        "  print('Validation acc: %s' % (float(val_acc),))\n",
        "  end_epoch = time.time()\n",
        "  print(\"time taken for this epoch(in seconds): \",(end_epoch - start_epoch))\n",
        "stop = time.time()\n",
        "print(\"total time(in second): \", (stop - start))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== epoch number: 0\n",
            "Training acc over epoch: 0.25905999541282654\n",
            "Validation acc: 0.3402999937534332\n",
            "time taken for this epoch(in seconds):  9.754390001296997\n",
            "===== epoch number: 1\n",
            "Training acc over epoch: 0.39362001419067383\n",
            "Validation acc: 0.4287000000476837\n",
            "time taken for this epoch(in seconds):  9.653002977371216\n",
            "===== epoch number: 2\n",
            "Training acc over epoch: 0.45673999190330505\n",
            "Validation acc: 0.4528999924659729\n",
            "time taken for this epoch(in seconds):  9.791287422180176\n",
            "===== epoch number: 3\n",
            "Training acc over epoch: 0.5008400082588196\n",
            "Validation acc: 0.516700029373169\n",
            "time taken for this epoch(in seconds):  9.708388090133667\n",
            "===== epoch number: 4\n",
            "Training acc over epoch: 0.5558000206947327\n",
            "Validation acc: 0.5702000260353088\n",
            "time taken for this epoch(in seconds):  9.734238862991333\n",
            "===== epoch number: 5\n",
            "Training acc over epoch: 0.6041600108146667\n",
            "Validation acc: 0.5964000225067139\n",
            "time taken for this epoch(in seconds):  9.706636428833008\n",
            "===== epoch number: 6\n",
            "Training acc over epoch: 0.6413400173187256\n",
            "Validation acc: 0.6341999769210815\n",
            "time taken for this epoch(in seconds):  9.727236032485962\n",
            "===== epoch number: 7\n",
            "Training acc over epoch: 0.6724200248718262\n",
            "Validation acc: 0.6330999732017517\n",
            "time taken for this epoch(in seconds):  9.752601861953735\n",
            "===== epoch number: 8\n",
            "Training acc over epoch: 0.6953799724578857\n",
            "Validation acc: 0.6528000235557556\n",
            "time taken for this epoch(in seconds):  9.804837465286255\n",
            "===== epoch number: 9\n",
            "Training acc over epoch: 0.7173200249671936\n",
            "Validation acc: 0.6554999947547913\n",
            "time taken for this epoch(in seconds):  9.719572305679321\n",
            "===== epoch number: 10\n",
            "Training acc over epoch: 0.7442600131034851\n",
            "Validation acc: 0.6513000130653381\n",
            "time taken for this epoch(in seconds):  9.761372089385986\n",
            "===== epoch number: 11\n",
            "Training acc over epoch: 0.7710999846458435\n",
            "Validation acc: 0.6525999903678894\n",
            "time taken for this epoch(in seconds):  9.746486902236938\n",
            "===== epoch number: 12\n",
            "Training acc over epoch: 0.7865599989891052\n",
            "Validation acc: 0.6536999940872192\n",
            "time taken for this epoch(in seconds):  9.734984636306763\n",
            "===== epoch number: 13\n",
            "Training acc over epoch: 0.8042799830436707\n",
            "Validation acc: 0.652400016784668\n",
            "time taken for this epoch(in seconds):  9.706164121627808\n",
            "===== epoch number: 14\n",
            "Training acc over epoch: 0.8299000263214111\n",
            "Validation acc: 0.6523000001907349\n",
            "time taken for this epoch(in seconds):  9.71235203742981\n",
            "===== epoch number: 15\n",
            "Training acc over epoch: 0.8465999960899353\n",
            "Validation acc: 0.6504999995231628\n",
            "time taken for this epoch(in seconds):  9.720499515533447\n",
            "===== epoch number: 16\n",
            "Training acc over epoch: 0.8636599779129028\n",
            "Validation acc: 0.6474000215530396\n",
            "time taken for this epoch(in seconds):  9.704897403717041\n",
            "===== epoch number: 17\n",
            "Training acc over epoch: 0.8754799962043762\n",
            "Validation acc: 0.6438000202178955\n",
            "time taken for this epoch(in seconds):  9.709655046463013\n",
            "===== epoch number: 18\n",
            "Training acc over epoch: 0.8928999900817871\n",
            "Validation acc: 0.6556000113487244\n",
            "time taken for this epoch(in seconds):  9.756669759750366\n",
            "===== epoch number: 19\n",
            "Training acc over epoch: 0.904259979724884\n",
            "Validation acc: 0.6366999745368958\n",
            "time taken for this epoch(in seconds):  9.683847665786743\n",
            "===== epoch number: 20\n",
            "Training acc over epoch: 0.9186999797821045\n",
            "Validation acc: 0.6353999972343445\n",
            "time taken for this epoch(in seconds):  9.742573022842407\n",
            "total time(in second):  204.3338918685913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAC-vrVQe_wB",
        "colab_type": "text"
      },
      "source": [
        "Conclusion: with tf.function computation was faster than regular function. with tf.function it took 176.19 seconds in total for 21 epochs(approximately 8.35 seconds per epoch) where as with out tf.function it took 204.33.50 seconds. i.e approximately 9.7 seconds per epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9to98r7NKbO1",
        "colab_type": "text"
      },
      "source": [
        "### Dense Network on cifar10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dUbTG8DKjl5",
        "colab_type": "code",
        "outputId": "56c872c0-d9dd-4ad0-e690-c93023c46db2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2ab9lQUfvCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 250\n",
        "l = 5\n",
        "num_filter = 36\n",
        "compression = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epr9q8geP_gg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LT6sxc2f7rD",
        "colab_type": "code",
        "outputId": "b2f32c54-38f2-472f-c7b5-7a5249e4d8e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"training samples shape: \", x_train.shape)\n",
        "print(\"training labels shape: \", y_train.shape)\n",
        "print(\"test samples shape: \", x_test.shape)\n",
        "print(\"test labels shape: \", y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training samples shape:  (50000, 32, 32, 3)\n",
            "training labels shape:  (50000, 10)\n",
            "test samples shape:  (10000, 32, 32, 3)\n",
            "test labels shape:  (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKn4v0iUgI5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape = (x_train.shape[1],x_train.shape[2],x_train.shape[3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-AHbMcNhBxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dense_block(input,num_of_filters):\n",
        "  global compression\n",
        "  temp = input\n",
        "  for _ in range(l):\n",
        "    BatchNorm_BN = BatchNormalization()(temp)\n",
        "    relu_BN = Activation('relu')(BatchNorm_BN)\n",
        "    conv_2d_1_1_BN = Conv2D(int(num_of_filters),(1,1),padding='same',kernel_initializer='he_uniform')(relu_BN)\n",
        "    BatchNorm = BatchNormalization()(conv_2d_1_1_BN)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    conv_2d_3_3 = Conv2D(int(num_of_filters),(3,3),padding='same',kernel_initializer='he_uniform')(relu)\n",
        "    concat = concatenate([temp,conv_2d_3_3],axis=-1)\n",
        "    temp = concat\n",
        "  return temp\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Tvt5f-Wkxf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transition_block(input,num_of_filters):\n",
        "  global compression\n",
        "  BatchNorm = BatchNormalization()(input)\n",
        "  relu = Activation('relu')(BatchNorm)\n",
        "  conv_2d_1_1 = Conv2D(int(num_of_filters*compression),(1,1),padding='same',kernel_initializer='he_uniform')(relu)\n",
        "  AveragePool = AveragePooling2D(pool_size=(2, 2), strides=2)(conv_2d_1_1)\n",
        "  return AveragePool"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFKNZTwcm4_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    AveragePooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    conv_2d_2_2 = Conv2D(num_classes, kernel_size = (2,2))(AveragePooling)\n",
        "    output = Activation('softmax')(conv_2d_2_2)\n",
        "    flat = Flatten()(output)\n",
        "    return flat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SwqCjZfox5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = Input(shape=input_shape)\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), kernel_initializer='he_uniform' ,padding='same')(input)\n",
        "\n",
        "First_Block = dense_block(First_Conv2D, num_filter)\n",
        "First_Transition = transition_block(First_Block, num_filter)\n",
        "\n",
        "Second_Block = dense_block(First_Transition, num_filter)\n",
        "Second_Transition = transition_block(Second_Block, num_filter)\n",
        "\n",
        "Third_Block = dense_block(Second_Transition, num_filter)\n",
        "Third_Transition = transition_block(Third_Block, num_filter)\n",
        "\n",
        "Last_Block = dense_block(Third_Transition,num_filter)\n",
        "output = output_layer(Last_Block)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQTxoVrwqRUE",
        "colab_type": "code",
        "outputId": "c0268860-9e0e-4c51-d934-9b9cc0ac2a80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = models.Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 36)   1008        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 36)   144         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 36)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 36)   1332        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 36)   144         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 36)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 36)   11700       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 32, 32, 72)   0           conv2d[0][0]                     \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 72)   288         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 72)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 36)   2628        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 36)   144         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 36)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 36)   11700       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 108)  0           concatenate[0][0]                \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 108)  432         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 108)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 36)   3924        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 36)   144         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 36)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 36)   11700       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 144)  0           concatenate_1[0][0]              \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 144)  576         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 144)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 36)   5220        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 36)   144         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 36)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 36)   11700       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 180)  0           concatenate_2[0][0]              \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 180)  720         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 180)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 36)   6516        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 36)   144         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 36)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 36)   11700       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 216)  0           concatenate_3[0][0]              \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 216)  864         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 216)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 18)   3906        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 16, 16, 18)   0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 18)   72          average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 18)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 36)   684         activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 36)   144         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 36)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 36)   11700       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 16, 16, 54)   0           average_pooling2d[0][0]          \n",
            "                                                                 conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 54)   216         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 54)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 36)   1980        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 36)   144         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 36)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 36)   11700       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 16, 16, 90)   0           concatenate_5[0][0]              \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 90)   360         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 90)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 36)   3276        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 36)   144         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 36)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 36)   11700       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 16, 16, 126)  0           concatenate_6[0][0]              \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 126)  504         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 126)  0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 36)   4572        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 36)   144         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 36)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 36)   11700       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 16, 16, 162)  0           concatenate_7[0][0]              \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 162)  648         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 162)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 36)   5868        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 36)   144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 36)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 36)   11700       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 16, 16, 198)  0           concatenate_8[0][0]              \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 198)  792         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 198)  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 18)   3582        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 8, 8, 18)     0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 8, 8, 18)     72          average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 8, 8, 18)     0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 8, 8, 36)     684         activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 8, 8, 36)     144         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 8, 8, 36)     0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 8, 8, 36)     11700       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 8, 8, 54)     0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 8, 8, 54)     216         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 8, 8, 54)     0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 8, 8, 36)     1980        activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 8, 8, 36)     144         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 8, 8, 36)     0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 8, 8, 36)     11700       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 8, 8, 90)     0           concatenate_10[0][0]             \n",
            "                                                                 conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 8, 90)     360         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 8, 8, 90)     0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 8, 8, 36)     3276        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 36)     144         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 36)     0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 36)     11700       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 8, 8, 126)    0           concatenate_11[0][0]             \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 8, 126)    504         concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 8, 126)    0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 36)     4572        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 36)     144         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 8, 36)     0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 36)     11700       activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 8, 8, 162)    0           concatenate_12[0][0]             \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 8, 162)    648         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 162)    0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 36)     5868        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 8, 36)     144         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 36)     0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 36)     11700       activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 8, 8, 198)    0           concatenate_13[0][0]             \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 8, 198)    792         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 198)    0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 18)     3582        activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 4, 4, 18)     0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 4, 4, 18)     72          average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 4, 4, 18)     0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 4, 4, 36)     684         activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 4, 4, 36)     144         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 4, 4, 36)     0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 4, 4, 36)     11700       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 4, 4, 54)     0           average_pooling2d_2[0][0]        \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 4, 4, 54)     216         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 4, 4, 54)     0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 4, 4, 36)     1980        activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 4, 4, 36)     144         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 4, 4, 36)     0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 4, 4, 36)     11700       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 4, 4, 90)     0           concatenate_15[0][0]             \n",
            "                                                                 conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 4, 4, 90)     360         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 4, 4, 90)     0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 4, 4, 36)     3276        activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 4, 4, 36)     144         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 4, 4, 36)     0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 4, 4, 36)     11700       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 4, 4, 126)    0           concatenate_16[0][0]             \n",
            "                                                                 conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 4, 4, 126)    504         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 4, 4, 126)    0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 4, 4, 36)     4572        activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 4, 4, 36)     144         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 4, 4, 36)     0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 4, 4, 36)     11700       activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 4, 4, 162)    0           concatenate_17[0][0]             \n",
            "                                                                 conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 4, 4, 162)    648         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 4, 4, 162)    0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 4, 4, 36)     5868        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 4, 4, 36)     144         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 4, 4, 36)     0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 4, 4, 36)     11700       activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 4, 4, 198)    0           concatenate_18[0][0]             \n",
            "                                                                 conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 4, 4, 198)    792         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 4, 4, 198)    0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 2, 2, 198)    0           activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 1, 1, 10)     7930        average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 1, 1, 10)     0           conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 10)           0           activation_44[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 336,448\n",
            "Trainable params: 329,608\n",
            "Non-trainable params: 6,840\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc1aO4plsHoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = ImageDataGenerator(rotation_range = 15, horizontal_flip = True, width_shift_range = 0.1, height_shift_range = 0.1, zoom_range = 0.2, shear_range = 15)\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CPztJ2l1wrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(0.005),metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rsi-L5X15wT",
        "colab_type": "code",
        "outputId": "9ae2251b-9b2a-4bbe-c998-4f5e587764ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(datagen.flow(x_train, y_train, batch_size), steps_per_epoch = 3*x_train.shape[0]/batch_size, \n",
        "                    epochs = 30 ,validation_data =(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1172/1171 [==============================] - 98s 84ms/step - loss: 1.4072 - accuracy: 0.4939 - val_loss: 2.2393 - val_accuracy: 0.4292\n",
            "Epoch 2/30\n",
            "1172/1171 [==============================] - 96s 82ms/step - loss: 0.9123 - accuracy: 0.6768 - val_loss: 0.9321 - val_accuracy: 0.6844\n",
            "Epoch 3/30\n",
            "1172/1171 [==============================] - 96s 82ms/step - loss: 0.7124 - accuracy: 0.7498 - val_loss: 1.0173 - val_accuracy: 0.6874\n",
            "Epoch 4/30\n",
            "1172/1171 [==============================] - 96s 82ms/step - loss: 0.6011 - accuracy: 0.7897 - val_loss: 0.9142 - val_accuracy: 0.7159\n",
            "Epoch 5/30\n",
            "1172/1171 [==============================] - 96s 82ms/step - loss: 0.5282 - accuracy: 0.8165 - val_loss: 0.6595 - val_accuracy: 0.7963\n",
            "Epoch 6/30\n",
            "1172/1171 [==============================] - 96s 82ms/step - loss: 0.4779 - accuracy: 0.8328 - val_loss: 0.6896 - val_accuracy: 0.7734\n",
            "Epoch 7/30\n",
            "1172/1171 [==============================] - 96s 82ms/step - loss: 0.4393 - accuracy: 0.8461 - val_loss: 0.6521 - val_accuracy: 0.8033\n",
            "Epoch 8/30\n",
            "1172/1171 [==============================] - 96s 82ms/step - loss: 0.4090 - accuracy: 0.8571 - val_loss: 0.5876 - val_accuracy: 0.8129\n",
            "Epoch 9/30\n",
            "1172/1171 [==============================] - 96s 82ms/step - loss: 0.3834 - accuracy: 0.8665 - val_loss: 0.5471 - val_accuracy: 0.8297\n",
            "Epoch 10/30\n",
            "1172/1171 [==============================] - 96s 82ms/step - loss: 0.3617 - accuracy: 0.8746 - val_loss: 0.5397 - val_accuracy: 0.8260\n",
            "Epoch 11/30\n",
            "1172/1171 [==============================] - 96s 82ms/step - loss: 0.3461 - accuracy: 0.8791 - val_loss: 0.6263 - val_accuracy: 0.8206\n",
            "Epoch 12/30\n",
            "1172/1171 [==============================] - 96s 82ms/step - loss: 0.3270 - accuracy: 0.8860 - val_loss: 0.4880 - val_accuracy: 0.8469\n",
            "Epoch 13/30\n",
            "1172/1171 [==============================] - 98s 83ms/step - loss: 0.3143 - accuracy: 0.8898 - val_loss: 0.5088 - val_accuracy: 0.8349\n",
            "Epoch 14/30\n",
            "1172/1171 [==============================] - 96s 82ms/step - loss: 0.3000 - accuracy: 0.8955 - val_loss: 0.4155 - val_accuracy: 0.8685\n",
            "Epoch 15/30\n",
            "1172/1171 [==============================] - 96s 82ms/step - loss: 0.2894 - accuracy: 0.8991 - val_loss: 0.5673 - val_accuracy: 0.8302\n",
            "Epoch 16/30\n",
            "1172/1171 [==============================] - 96s 82ms/step - loss: 0.2788 - accuracy: 0.9023 - val_loss: 0.5822 - val_accuracy: 0.8377\n",
            "Epoch 17/30\n",
            "1172/1171 [==============================] - 100s 85ms/step - loss: 0.2688 - accuracy: 0.9058 - val_loss: 0.5555 - val_accuracy: 0.8384\n",
            "Epoch 18/30\n",
            "1172/1171 [==============================] - 97s 83ms/step - loss: 0.2593 - accuracy: 0.9088 - val_loss: 0.4666 - val_accuracy: 0.8609\n",
            "Epoch 19/30\n",
            "1172/1171 [==============================] - 96s 82ms/step - loss: 0.2504 - accuracy: 0.9112 - val_loss: 0.4060 - val_accuracy: 0.8744\n",
            "Epoch 20/30\n",
            "1172/1171 [==============================] - 96s 82ms/step - loss: 0.2454 - accuracy: 0.9138 - val_loss: 0.4500 - val_accuracy: 0.8619\n",
            "Epoch 21/30\n",
            "1172/1171 [==============================] - 97s 83ms/step - loss: 0.2378 - accuracy: 0.9160 - val_loss: 0.4623 - val_accuracy: 0.8689\n",
            "Epoch 22/30\n",
            "1172/1171 [==============================] - 99s 85ms/step - loss: 0.2297 - accuracy: 0.9197 - val_loss: 0.4539 - val_accuracy: 0.8672\n",
            "Epoch 23/30\n",
            "1172/1171 [==============================] - 97s 83ms/step - loss: 0.2263 - accuracy: 0.9203 - val_loss: 0.4272 - val_accuracy: 0.8726\n",
            "Epoch 24/30\n",
            "1172/1171 [==============================] - 97s 83ms/step - loss: 0.2185 - accuracy: 0.9231 - val_loss: 0.4358 - val_accuracy: 0.8744\n",
            "Epoch 25/30\n",
            "1172/1171 [==============================] - 97s 83ms/step - loss: 0.2118 - accuracy: 0.9247 - val_loss: 0.4750 - val_accuracy: 0.8588\n",
            "Epoch 26/30\n",
            "1172/1171 [==============================] - 97s 83ms/step - loss: 0.2092 - accuracy: 0.9254 - val_loss: 0.5069 - val_accuracy: 0.8615\n",
            "Epoch 27/30\n",
            "1172/1171 [==============================] - 97s 83ms/step - loss: 0.2058 - accuracy: 0.9270 - val_loss: 0.3669 - val_accuracy: 0.8918\n",
            "Epoch 28/30\n",
            "1172/1171 [==============================] - 97s 83ms/step - loss: 0.1986 - accuracy: 0.9303 - val_loss: 0.4294 - val_accuracy: 0.8765\n",
            "Epoch 29/30\n",
            "1172/1171 [==============================] - 97s 83ms/step - loss: 0.1943 - accuracy: 0.9313 - val_loss: 0.4185 - val_accuracy: 0.8845\n",
            "Epoch 30/30\n",
            "1172/1171 [==============================] - 96s 82ms/step - loss: 0.1887 - accuracy: 0.9333 - val_loss: 0.5100 - val_accuracy: 0.8634\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5961d679e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl-SN0OLTGCJ",
        "colab_type": "text"
      },
      "source": [
        "Conclusion: Achecived an accuracy of 89.50% with growth rate of 5, compression rate of 0.5 and leaarning rate of 0.005 with Adam Optimizier. We Made use of 4 dense blocks with data agumentation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tBkcP6sUmGz",
        "colab_type": "text"
      },
      "source": [
        "### Other Experimental results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsgEtQZxU9AI",
        "colab_type": "text"
      },
      "source": [
        "Summary: \n",
        "\n",
        "1. Achecived an accuracy of 87% with growth rate of 5, compression rate of 0.5 and leaarning rate of 0.005 with Adam Optimizier. We Made use of 4 dense blocks with data agumentation **with out bottle neck**.\n",
        "\n",
        "2. **Playing with growth rate**\n",
        "\n",
        "Summary: Earlier we saw densenets with growth rate of fixed 5. Now we will change the growth rate to 3, 7, 9 and 12 and run for 10 - 20 epochs(epochs are kept a little less as gpu availability need to be considered). As expected , increase in growth rate leads to increase in accuracy as more and more featuremaps are considered inside the dense block\n",
        "\n",
        "\n",
        "growth rate = 3 | config 4: Params used = 80K, time taken to train per epoch = 80 seconds, Training accuracy = 86%, Test accuracy = 84%\n",
        "\n",
        "\n",
        "growth rate = 7 | config 3: Params used = 230K, time taken to train per epoch = 100 seconds, Training accuracy = 90%, Test accuracy = 88%\n",
        "\n",
        "\n",
        "growth rate = 9 | config 5: Params used = 330K, time taken to train per epoch = 120 seconds, Training accuracy = 93%, Test accuracy = 90%\n",
        "\n",
        "\n",
        "growth rate = 12 | config 6: Params used = 450K, time taken to train per epoch = 150 seconds, Training accuracy = 94%, Test accuracy = 90%\n",
        "\n",
        "3. **Playing with compression**\n",
        "\n",
        "Summary: We have fixed growth rate to 5 and experiment with high(0.2), medium(0.5) and low compression(0.9). We recorded the accuracy , number of parameters and training time per epoch. We ran experiments for 10-20 epochs\n",
        "As expected if we use more compression, the parameters would decrease and training time also decreased meagerly. But accuracy actually remained same for compression = 0.2 when compared to 0.5 which was a surprise, maybe there were more redundancy in featuremaps that could be removed without affecting overall outcome. As expected for low compression(0.9) we got 87% accuracy.\n",
        "\n",
        "\n",
        "compression = 0.2 | config 7: Params used = 120K, time taken to train per epoch = 83 seconds, Training accuracy = 88%, Test accuracy = 86.5%\n",
        "\n",
        "\n",
        "compression = 0.5 | config 2: Params used = 150K, time taken to train per epoch = 85 seconds, Training accuracy = 89%, Test accuracy = 87.2%\n",
        "\n",
        "\n",
        "compression = 0.9 | config 8: Params used = 200K, time taken to train per epoch = 90 seconds, Training accuracy = 91%, Test accuracy = 87%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqxDawCrquJb",
        "colab_type": "text"
      },
      "source": [
        "### Computational Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD69nswNt6dD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorboard\n",
        "from datetime import datetime\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdeViGb4qzBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QthqHS7XaJZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test set\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    train_images = train_images.reshape(train_images.shape[0], 1, img_rows, img_cols)\n",
        "    test_images = test_images.reshape(test_images.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, 1)\n",
        "    test_images = test_images.reshape(test_images.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REitjZZBrNKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices(( train_images.astype(np.float32)/255.0, train_labels.astype(np.int32) ))\n",
        "\n",
        "train_data = train_data.shuffle(buffer_size = train_images.shape[0]).batch(32)\n",
        "\n",
        "test_data = tf.data.Dataset.from_tensor_slices(( test_images.astype(np.float32)/255.0, test_labels.astype(np.int32) ))\n",
        "\n",
        "test_data = test_data.batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Lo_cf-SrUsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([layers.Conv2D(64,(3,3),activation='relu',padding='same',input_shape=input_shape),\n",
        "                          layers.MaxPool2D(),\n",
        "                          layers.Conv2D(64,(3,3),activation='relu',padding='same'),\n",
        "                          layers.MaxPool2D(), \n",
        "                          layers.Flatten(), \n",
        "                          layers.Dense(64, activation='relu'), \n",
        "                          layers.Dense(10) ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49-Y7DA0riYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate an optimizer.\n",
        "optimizer = keras.optimizers.Adam(learning_rate=  5 * 1e-3)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Prepare the metrics.\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjSvOMvJrarM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(imgs, lbls):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(imgs)\n",
        "        xent = loss_fn(lbls, logits)\n",
        "\n",
        "    varis = model.trainable_variables\n",
        "    grads = tape.gradient(xent, varis)\n",
        "    optimizer.apply_gradients(zip(grads, varis))\n",
        "\n",
        "    return xent, logits\n",
        "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "logdir = 'logs/func/%s' % stamp\n",
        "writer = tf.summary.create_file_writer(logdir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz40dh75rwHr",
        "colab_type": "code",
        "outputId": "46ccfe4e-ae1b-493f-b863-5b34223e5fee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "epochs=5\n",
        "for epoch in range(epochs):\n",
        "  print(\"===== epoch number: {}\".format(epoch))\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_data):\n",
        "    tf.summary.trace_on(graph=True, profiler=True)\n",
        "    xent, logits = train_step(x_batch_train, y_batch_train)\n",
        "    with writer.as_default():\n",
        "      tf.summary.trace_export(\n",
        "      name=\"my_func_trace\",\n",
        "      step=0,\n",
        "      profiler_outdir=logdir)\n",
        "    train_acc_metric(y_batch_train, logits)\n",
        "\n",
        "  train_acc = train_acc_metric.result()\n",
        "  tf.print('Training acc over epoch: %s' % (float(train_acc),))\n",
        "  train_acc_metric.reset_states()\n",
        "\n",
        "  for x_batch_val, y_batch_val in test_data:\n",
        "    val_logits = model(x_batch_val)\n",
        "    val_acc_metric(y_batch_val, val_logits)\n",
        "  val_acc = val_acc_metric.result()\n",
        "  val_acc_metric.reset_states()\n",
        "  tf.print('Validation acc: %s' % (float(val_acc),))"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== epoch number: 0\n",
            "Training acc over epoch: 0.9647833108901978\n",
            "Validation acc: 0.9842000007629395\n",
            "===== epoch number: 1\n",
            "Training acc over epoch: 0.9840999841690063\n",
            "Validation acc: 0.9869999885559082\n",
            "===== epoch number: 2\n",
            "Training acc over epoch: 0.988016664981842\n",
            "Validation acc: 0.9854000210762024\n",
            "===== epoch number: 3\n",
            "Training acc over epoch: 0.9894333481788635\n",
            "Validation acc: 0.9872999787330627\n",
            "===== epoch number: 4\n",
            "Training acc over epoch: 0.9908000230789185\n",
            "Validation acc: 0.9904000163078308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H18bfqAZtxqb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bd43a968-caf7-4855-b3a5-00a32e8e9327"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mucozkdAtyfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs\n",
        "# tensorboard --logdir logs"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GU139r5qsCda"
   },
   "source": [
    "**Task 7:** What did you learn?\n",
    "\n",
    "**Team members:**\n",
    "1. Himanshi Bajaj: 225827  : himanshi.bajaj@st.ovgu.de\n",
    "2. Sanjeeth Busnur Indushekar: 224133 : sanjeeth.busnur@st.ovgu.de\n",
    "3. Suraj Shashidhar: 230052 : suraj.shashidhar@st.ovgu.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1q4op-DCsT1H"
   },
   "source": [
    "**Himanshi Bajaj**\n",
    "\n",
    "- **Which findings were just showing what you already expected?**\n",
    "\n",
    "  Ans.  1. For task 1, We tried ResNet and VGG net with different optimisers such as ADAM, SGD, RMSProp and Adagrad. As expected in VGG Net Architecture we got better results using ADAM Optimiser.\n",
    "   \n",
    " 2. For task 2, With Regularisation VGG architecture gave better accuracy on validation set, and helped in overcoming overfitting problem.\n",
    "\n",
    " 3. For Task 3, Transfer learning performed better when source domain was complex i.e. FMNIST and target domain was simpler i.e. , MNIST\n",
    "\n",
    "\n",
    "- **Were there any results which you find surprising or interesting?**\n",
    "\n",
    "   Ans.  For task 1, We expected ResNet to perform better as it had deeper network but VGGNet gave better accuracy.\n",
    "In ResNet Theoretically Loss function's value should decrease as the number of epochs increases but we can see the slight fluctuation in loss function's value.\n",
    "\n",
    "- **Which tasks were easy for you to solve?**\n",
    "\n",
    "   Ans. Task 1, 2 and 3 were pretty easy to solve.\n",
    "\n",
    "- **Which tasks were difficult and what would you have needed to solve them (better)?**\n",
    "\n",
    "   Ans.  Task 4,5,6 took more time to solve, as Neural Machine Translation and Bert we studied but didn't implement during the course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6atYvwi_spWc"
   },
   "source": [
    "**Sanjeeth Busnur Indushekar**\n",
    "\n",
    "- **Which findings were just showing what you already expected?**\n",
    "\n",
    "  Ans.  1. For task 2, We tried Implementing VGG and RESNET with and with out regularisation.\n",
    "We tried out Regularisation techniques such as Drop out, Data Augmentation, L2 regularisation\n",
    "As expected with Regularisation in VGG architecture we were able to get better accuracy on validation set, i.e Regularisation helped in overcoming overfitting problem but in ResNet, Validation set got better accuracy with out regularisation techniques and reason for this might be the network is too deeper.\n",
    "\n",
    "  2. For task 3.3, first two subparts gave poor results which was expected as layers were non trainable, but for 4th subpart, replacing source classification layer with multiple layers of target domain gave good results.\n",
    "  \n",
    "- **Were there any results which you find surprising or interesting?**\n",
    "\n",
    "   Ans.  1. For Task 1, We tried ResNet and VGG net with different optimisers such as ADAM, SGD, RMSProp and Adagrad Optimiser.Surprisingly, Adagrad Optimiser worked better and gave better results in ResNet Architecture.\n",
    "\n",
    "- **Which tasks were easy for you to solve?**\n",
    "\n",
    "   Ans. Task 1,2 were easy to solve.\n",
    "\n",
    "- **Which tasks were difficult and what would you have needed to solve them (better)?**\n",
    "\n",
    "   Ans. Task 3,4,5 and 6 were difficult and more time consuming to solve as well as train. More network resources could have helped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSSa5M0wsuYL"
   },
   "source": [
    "**Suraj Shashidhar**\n",
    "\n",
    "\n",
    "- **Which findings were just showing what you already expected?**\n",
    "\n",
    "  Ans. Attention distribution in Neural machine translation for french to english were as expected for some sentences. Eventhough it deviated for some sentences.\n",
    "\n",
    "- **Were there any results which you find surprising or interesting?**\n",
    "  \n",
    "  Ans. I found results in Introspection Sensitivity Analysis to be interesting, Escpecially was surprised to see the pixels which were mostly responsible for classifying the MNSIT digit is not sensible at all. \n",
    "  \n",
    "  The way neural network classified a digit is radically different than how humans would classify the digit and machines are not smarter than human vision even though it could have more accuracy in certain tasks than human.\n",
    "\n",
    "- **Which tasks were easy for you to solve?**\n",
    "\n",
    "   Ans. None of the tasks were easy.\n",
    "\n",
    "- **Which tasks were difficult and what would you have needed to solve them (better)?**\n",
    "\n",
    "   Ans. All of them were difficult, Eventhough I was clear on concepts I felt the need for more training on tensorflow to solve them easily."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project Assignment Task 7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
